# RL_Based_MultiAgent_Trading_Systems
Multi-Agent Deep Q-Learning Based Trading System with Centralized Reward Optimization
 
ğŸ§© Project Overview:
This project aims to build a multi-agent reinforcement learning (MARL) trading system where each agent is powered by a Deep Q-Network (DQN). Each agent learns to trade independently in a simulated market environment using candlestick chart data and technical indicators. The system applies a centralized reward mechanism to encourage cooperative strategies that collectively maximize portfolio profit. Agents are trained on historical market data and evaluated on back testing and live data streams.

ğŸ“ Project Folder: RL_Based_MultiAgent_Trading_Systems/
multiagent-dqn-trader/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚   â””â”€â”€ model.py
â”‚
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ trading_env.py
â”‚   â”œâ”€â”€ market_simulator.py
â”‚   â””â”€â”€ reward_functions.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ download_data.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_agents.py
â”‚   â”œâ”€â”€ centralized_reward_tracker.py
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ backtest.py
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ live_trading.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ indicators.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ plotter.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ (Saved model .pt/.h5 files here)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸ“ Folders & Key Files
agents/

dqn_agent.py â€“ DQN agent class (Q-network, epsilon-greedy, action selection)
replay_buffer.py â€“ Experience Replay memory buffer
model.py â€“ Neural network architecture (e.g., CNN or MLP for Q-value estimation)
env/

trading_env.py â€“ Gym-compatible environment for trading simulation
market_simulator.py â€“ Logic for trade execution, position management, market stepping
reward_functions.py â€“ Defines local and centralized reward structures
data/

download_data.py â€“ Script to fetch data from APIs (e.g., Yahoo Finance, Binance)
raw/, processed/ â€“ Store historical and processed data
training/

train_agents.py â€“ Main training loop for multiple agents
centralized_reward_tracker.py â€“ Tracks and computes shared rewards
config.yaml â€“ Configuration file (hyperparameters, model settings, etc.)
evaluation/

evaluate.py â€“ Runs trained models on test data
backtest.py â€“ Backtesting engine to simulate and visualize agent trades
deployment/

api.py â€“ Optional REST API for live inference
live_trading.py â€“ Integrates with a broker for real-time deployment (e.g., Binance API)
utils/

indicators.py â€“ Add RSI, MACD, EMA, Bollinger Bands to data
logger.py â€“ Logging for training and evaluation
plotter.py â€“ Trade visualization and performance plots
models/

Store .pt or .h5 files for trained agents.
notebooks/

EDA.ipynb â€“ Exploratory Data Analysis on market data
Root Files

requirements.txt â€“ List of dependencies (gym, pytorch, numpy, etc.)
README.md â€“ Project overview, setup instructions, usage
.gitignore â€“ Ignore model files, logs, virtual environments


âœ… Starter Commands
# Install dependencies
pip install -r requirements.txt

# Train agents
python training/train_agents.py

# Evaluate agents
python evaluation/evaluate.py

# Backtest
python evaluation/backtest.py


